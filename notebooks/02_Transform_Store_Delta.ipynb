{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a76d24e",
   "metadata": {},
   "source": [
    "### üß± Step 1: Initialize SparkSession\n",
    "\n",
    "We start by creating the Spark session with Delta support enabled to process and store the crypto data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615c2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark with cleaner logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transform and Save Delta\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # üëà Hide warnings like SparkUI port binding, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9046b",
   "metadata": {},
   "source": [
    "### üß¨ Step 2: Convert Pandas to Spark DataFrame\n",
    "\n",
    "We load the pandas DataFrame using the API function and convert it into a distributed Spark DataFrame for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da1dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+--------------------+-----------------+\n",
      "|           timestamp|            price|\n",
      "+--------------------+-----------------+\n",
      "|2025-04-10 19:05:...|79529.67606306932|\n",
      "|2025-04-10 20:09:...|79724.89465979146|\n",
      "|2025-04-10 21:08:...|79878.62357184727|\n",
      "|2025-04-10 22:05:...|79709.36877328751|\n",
      "|2025-04-10 23:04:...|  79715.508089801|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from api_utils import get_market_data\n",
    "\n",
    "btc_df = get_market_data(\"bitcoin\", \"usd\", 30)\n",
    "\n",
    "# Convert to Spark\n",
    "btc_spark_df = spark.createDataFrame(btc_df)\n",
    "\n",
    "btc_spark_df.printSchema()\n",
    "btc_spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e06fa",
   "metadata": {},
   "source": [
    "### üßπ Step 3: Data Cleaning and Type Casting\n",
    "\n",
    "We ensure that columns have appropriate types for analysis and storage in Delta format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1f77ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+--------------------+-----------------+\n",
      "|           timestamp|            price|\n",
      "+--------------------+-----------------+\n",
      "|2025-04-10 19:05:...|79529.67606306932|\n",
      "|2025-04-10 20:09:...|79724.89465979146|\n",
      "|2025-04-10 21:08:...|79878.62357184727|\n",
      "|2025-04-10 22:05:...|79709.36877328751|\n",
      "|2025-04-10 23:04:...|  79715.508089801|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "btc_spark_df_clean = btc_spark_df.select(\n",
    "    col(\"timestamp\").cast(\"timestamp\"),\n",
    "    col(\"price\").cast(\"double\")\n",
    ")\n",
    "\n",
    "btc_spark_df_clean.printSchema()\n",
    "btc_spark_df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43ad9d",
   "metadata": {},
   "source": [
    "### üíæ Step 4: Save Cleaned Data to Delta Lake\n",
    "\n",
    "Finally, we store the transformed DataFrame in the Delta format within Microsoft Fabric's default lakehouse location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc52a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not write to Delta format. This step requires Microsoft Fabric or Delta Lake setup.\n",
      "\n",
      "Error details:\n",
      " An error occurred while calling o134.save.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.ClassNotFoundException: delta.DefaultSource\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
      "\tat scala.util.Failure.orElse(Try.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
      "\t... 16 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ü™£ Try saving to Delta (only works on Fabric or local Delta setup)\n",
    "try:\n",
    "    btc_spark_df_clean.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"Tables/bitcoin\")\n",
    "\n",
    "    print(\"‚úÖ Data written to Delta successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not write to Delta format. This step requires Microsoft Fabric or Delta Lake setup.\\n\")\n",
    "    print(\"Error details:\\n\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
